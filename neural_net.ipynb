{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.optimize\n",
    "import itertools\n",
    "\n",
    "#student configs\n",
    "data_path = \"./data/\"\n",
    "DEBUG = True\n",
    "# lr = 0.5\n",
    "# epochs = 5\n",
    "# batch = 50\n",
    "\n",
    "#given configs\n",
    "NUM_INPUT = 784  # Number of input neurons\n",
    "# global NUM_HIDDEN\n",
    "NUM_HIDDEN = 40  # Number of hidden neurons\n",
    "NUM_OUTPUT = 10  # Number of output neurons\n",
    "NUM_CHECK = 5  # Number of examples on which to check the gradient\n",
    "\n",
    "# Given a vector w containing all the weights and biased vectors, extract\n",
    "# and return the individual weights and biases W1, b1, W2, b2.\n",
    "# This is useful for performing a gradient check with check_grad.\n",
    "def unpack (w):\n",
    "    first_layer = NUM_HIDDEN*NUM_INPUT\n",
    "    second_layer = first_layer+NUM_HIDDEN\n",
    "    third_layer = second_layer+(NUM_OUTPUT*NUM_HIDDEN)\n",
    "    W1 = np.array(w[0:first_layer]).reshape(NUM_HIDDEN, NUM_INPUT)\n",
    "    b1 = np.array(w[first_layer:second_layer])\n",
    "    W2 = np.array(w[second_layer:third_layer]).reshape(NUM_OUTPUT, NUM_HIDDEN)\n",
    "    b2 = np.array(w[third_layer::])\n",
    "    return W1, b1, W2, b2\n",
    "\n",
    "# Given individual weights and biases W1, b1, W2, b2, concatenate them and\n",
    "# return a vector w containing all of them.\n",
    "# This is useful for performing a gradient check with check_grad.\n",
    "def pack (W1, b1, W2, b2):\n",
    "    W1 = W1.flatten()\n",
    "    W2 = W2.flatten()\n",
    "    w = np.concatenate((W1, b1, W2, b2))\n",
    "    # print(set(W1 == w[0:31360]))\n",
    "    # print(set(b1 == w[31360:31360+40]))\n",
    "    # print(set(W2 == w[31400:31800]))\n",
    "    # print(set(b2 == w[31800::]))\n",
    "    return w\n",
    "\n",
    "# Load the images and labels from a specified dataset (train or test).\n",
    "def loadData (which):\n",
    "    images = np.load(\"{}mnist_{}_images.npy\".format(data_path,which))\n",
    "    labels = np.load(\"{}mnist_{}_labels.npy\".format(data_path,which))\n",
    "    return images, labels\n",
    "\n",
    "# Given training images X, associated labels Y, and a vector of combined weights\n",
    "# and bias terms w, compute and return the cross-entropy (CE) loss. You might\n",
    "# want to extend this function to return multiple arguments (in which case you\n",
    "# will also need to modify slightly the gradient check code below).\n",
    "def fCE (X, Y, w):\n",
    "    (W1, b1, W2, b2) = unpack(w) #unpack(w)\n",
    "    z1 = compute_z(W1, X, b1)\n",
    "    h1 = reLU(z1)\n",
    "    z2 = compute_z(W2, h1, b2)\n",
    "    Yhat = softmax(z2)\n",
    "    cost = -np.mean(np.log(Yhat[Y==1]))\n",
    "    return cost\n",
    "\n",
    "def compute_z(W, x, b):\n",
    "    return np.dot(W, x.T).T + b\n",
    "\n",
    "def reLU(s):\n",
    "    return max(0, s)\n",
    "\n",
    "def reLUPrime(s):\n",
    "    return s>0\n",
    "\n",
    "def predictor(X, Y, w):\n",
    "    (W1, b1, W2, b2) = unpack(w) #unpack(w)\n",
    "    z1 = compute_z(W1, X, b1)\n",
    "    h1 = reLU(z1)\n",
    "    z2 = compute_z(W2, h1, b2)\n",
    "    Yhat = softmax(z2)\n",
    "    return Yhat\n",
    "\n",
    "reLU = np.vectorize(reLU) # vectorizing the i/o by applying reLU func\n",
    "reLUPrime = np.vectorize(reLUPrime) # vectorizing the i/o by applying reLUPrime func\n",
    "\n",
    "def softmax(z):\n",
    "    e_z = np.exp(z) \n",
    "    return (e_z.T/np.sum(e_z, axis=1).T).T\n",
    "\n",
    "def fPC(yhat, y):\n",
    "    return np.mean(yhat.argmax(axis=1) == y.argmax(axis=1))\n",
    "\n",
    "# Given training images X, associated labels Y, and a vector of combined weights\n",
    "# and bias terms w, compute and return the gradient of fCE. You might\n",
    "# want to extend this function to return multiple arguments (in which case you\n",
    "# will also need to modify slightly the gradient check code below).\n",
    "def gradCE (X, Y, w):\n",
    "    (W1, b1, W2, b2) = unpack(w) #unpack(w)\n",
    "    z1 = compute_z(W1, X, b1) \n",
    "    h1 = reLU(z1)\n",
    "    z2 = compute_z(W2, h1, b2)\n",
    "    Yhat = softmax(z2)\n",
    "    n = Y.shape[1]\n",
    "\n",
    "    g = ((Yhat-Y).dot(W2)) * reLUPrime(z1)\n",
    "    dW1 = g.T.dot(X) * (1/X.shape[0])\n",
    "    db1 = np.mean(g, axis=0)\n",
    "    dW2 = (Yhat-Y).T.dot(h1) * (1/X.shape[0])\n",
    "    db2 = np.mean(Yhat - Y, axis=0)\n",
    "    # print(dW1.shape, db1.shape, dW2.shape, db2.shape)\n",
    "    return pack(dW1,db1,dW2,db2)\n",
    "\n",
    "\n",
    "# Given training and testing datasets and an initial set of weights/biases b,\n",
    "# train the NN. Then return the sequence of w's obtained during SGD.\n",
    "def train (trainX, trainY, testX, testY, w, hyperparams):\n",
    "    (lr, batch, epochs) = hyperparams\n",
    "    (W1, b1, W2, b2) = unpack(w)\n",
    "    n = trainY.shape[0]\n",
    "    metrics = []\n",
    "    for i in range(epochs):\n",
    "        for minibatch in range(n//batch):\n",
    "            rX = trainX[minibatch*batch:(minibatch+1)*batch]\n",
    "            rY = trainY[minibatch*batch:(minibatch+1)*batch]\n",
    "            grads = gradCE(rX, rY, w)\n",
    "            (dW1, db1, dW2, db2) = unpack(grads)\n",
    "            W1 = W1 - lr*dW1\n",
    "            b1 = b1 - lr*db1\n",
    "            W2 = W2 - lr*dW2\n",
    "            b2 = b2 - lr*db2\n",
    "            w = pack(W1, b1, W2, b2)\n",
    "\n",
    "        yhat = predictor(testX, testY, w)\n",
    "        accuracy = fPC(yhat, testY)\n",
    "        cost = fCE(testX, testY, w)\n",
    "        lr = lr*0.95\n",
    "        \n",
    "        metrics.append([accuracy, cost])\n",
    "\n",
    "        if DEBUG and i+1 % 10 == 0:\n",
    "            print(\"----------------------\")\n",
    "            print(\"Epoch: {} ; Percent Correct Accuracy: {} ; Cross Entropy Loss: {}\".format(i+1, accuracy, cost))\n",
    "            \n",
    "        if (accuracy > 0.98 and cost < 0.16):\n",
    "            print(\"-------Last Epoch-----\")\n",
    "            print(\"Epoch: {} ; Percent Correct Accuracy: {} ; Cross Entropy Loss: {}\".format(i+1, accuracy, cost))\n",
    "            print(\"-------Last Epoch-----\")\n",
    "            break\n",
    "            \n",
    "    return (w, metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findBestHyperparameters():\n",
    "    global NUM_HIDDEN\n",
    "    hidden_range = [30, 40, 50]\n",
    "    lr_range     = [0.1, 0.5, 0.75]\n",
    "    batch_range  = [50, 200]\n",
    "    epochs       = [100]\n",
    "\n",
    "    results = []\n",
    "    for combo in itertools.product(hidden_range, lr_range, batch_range, epochs):\n",
    "\n",
    "        NUM_HIDDEN = combo[0]\n",
    "\n",
    "        W1 = 2*(np.random.random(size=(NUM_INPUT, NUM_HIDDEN))/NUM_INPUT**0.5) - 1./NUM_INPUT**0.5\n",
    "        b1 = 0.01 * np.ones(NUM_HIDDEN)\n",
    "        W2 = 2*(np.random.random(size=(NUM_HIDDEN, NUM_OUTPUT))/NUM_HIDDEN**0.5) - 1./NUM_HIDDEN**0.5\n",
    "        b2 = 0.01 * np.ones(NUM_OUTPUT)\n",
    "\n",
    "        W1, W2 = W1.T, W2.T\n",
    "        w = pack(W1, b1, W2, b2)\n",
    "\n",
    "        hps = combo[1:]\n",
    "        print('\\t(num_hidden, lr, batch, epochs)')\n",
    "        print(f'MODEL: {combo}')\n",
    "        ws = train(trainX, trainY, testX, testY, w, hps)\n",
    "        results.append([hps, ws])\n",
    "        \n",
    "    # TODO extract results for each model at epoch 20, 50, and 100 and compare mathematically\n",
    "    # return best hyperparams and TOTAL results list for charting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t(num_hidden, lr, batch, epochs)\n",
      "MODEL: (30, 0.1, 50, 100)\n",
      "\t(num_hidden, lr, batch, epochs)\n",
      "MODEL: (30, 0.1, 200, 100)\n",
      "\t(num_hidden, lr, batch, epochs)\n",
      "MODEL: (30, 0.5, 50, 100)\n",
      "\t(num_hidden, lr, batch, epochs)\n",
      "MODEL: (30, 0.5, 200, 100)\n",
      "\t(num_hidden, lr, batch, epochs)\n",
      "MODEL: (30, 0.75, 50, 100)\n",
      "\t(num_hidden, lr, batch, epochs)\n",
      "MODEL: (30, 0.75, 200, 100)\n",
      "\t(num_hidden, lr, batch, epochs)\n",
      "MODEL: (40, 0.1, 50, 100)\n",
      "\t(num_hidden, lr, batch, epochs)\n",
      "MODEL: (40, 0.1, 200, 100)\n",
      "\t(num_hidden, lr, batch, epochs)\n",
      "MODEL: (40, 0.5, 50, 100)\n",
      "\t(num_hidden, lr, batch, epochs)\n",
      "MODEL: (40, 0.5, 200, 100)\n",
      "\t(num_hidden, lr, batch, epochs)\n",
      "MODEL: (40, 0.75, 50, 100)\n",
      "\t(num_hidden, lr, batch, epochs)\n",
      "MODEL: (40, 0.75, 200, 100)\n",
      "\t(num_hidden, lr, batch, epochs)\n",
      "MODEL: (50, 0.1, 50, 100)\n",
      "\t(num_hidden, lr, batch, epochs)\n",
      "MODEL: (50, 0.1, 200, 100)\n",
      "\t(num_hidden, lr, batch, epochs)\n",
      "MODEL: (50, 0.5, 50, 100)\n",
      "\t(num_hidden, lr, batch, epochs)\n",
      "MODEL: (50, 0.5, 200, 100)\n",
      "\t(num_hidden, lr, batch, epochs)\n",
      "MODEL: (50, 0.75, 50, 100)\n",
      "\t(num_hidden, lr, batch, epochs)\n",
      "MODEL: (50, 0.75, 200, 100)\n"
     ]
    }
   ],
   "source": [
    "if \"trainX\" not in globals():\n",
    "    trainX, trainY = loadData(\"train\")\n",
    "    testX, testY = loadData(\"validation\")\n",
    "NUM_HIDDEN = 0    \n",
    "models = findBestHyperparameters()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "npws = np.array(ws[1])\n",
    "\n",
    "plt.plot(npws[:,0])\n",
    "plt.show()\n",
    "plt.plot(npws[:,1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "if \"trainX\" not in globals():\n",
    "    trainX, trainY = loadData(\"train\")\n",
    "    testX, testY = loadData(\"test\")\n",
    "\n",
    "# Initialize weights randomly\n",
    "W1 = 2*(np.random.random(size=(NUM_INPUT, NUM_HIDDEN))/NUM_INPUT**0.5) - 1./NUM_INPUT**0.5\n",
    "b1 = 0.01 * np.ones(NUM_HIDDEN)\n",
    "W2 = 2*(np.random.random(size=(NUM_HIDDEN, NUM_OUTPUT))/NUM_HIDDEN**0.5) - 1./NUM_HIDDEN**0.5\n",
    "b2 = 0.01 * np.ones(NUM_OUTPUT)\n",
    "\n",
    "W1 = W1.T\n",
    "W2 = W2.T\n",
    "w = pack(W1, b1, W2, b2)\n",
    "\n",
    "# Check that the gradient is correct on just a few examples (randomly drawn).\n",
    "idxs = np.random.permutation(trainX.shape[0])[0:NUM_CHECK]\n",
    "\n",
    "\n",
    "print(scipy.optimize.check_grad(lambda w_: fCE(np.atleast_2d(trainX[idxs,:]), np.atleast_2d(trainY[idxs,:]), w_), \\\n",
    "                                lambda w_: gradCE(np.atleast_2d(trainX[idxs,:]), np.atleast_2d(trainY[idxs,:]), w_), \\\n",
    "                                w))\n",
    "\n",
    "# Train the network and obtain the sequence of w's obtained using SGD.\n",
    "# ws = train(trainX, trainY, testX, testY, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
