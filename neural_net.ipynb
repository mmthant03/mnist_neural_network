{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.optimize\n",
    "\n",
    "#student configs\n",
    "data_path = \"./data/\"\n",
    "DEBUG = True\n",
    "\n",
    "#given configs\n",
    "NUM_INPUT = 784  # Number of input neurons\n",
    "NUM_HIDDEN = 40  # Number of hidden neurons\n",
    "NUM_OUTPUT = 10  # Number of output neurons\n",
    "NUM_CHECK = 5  # Number of examples on which to check the gradient\n",
    "\n",
    "# Given a vector w containing all the weights and biased vectors, extract\n",
    "# and return the individual weights and biases W1, b1, W2, b2.\n",
    "# This is useful for performing a gradient check with check_grad.\n",
    "def unpack (w):\n",
    "    W1, b1, W2, b2 = w\n",
    "\n",
    "    print(\"unpacking\")\n",
    "    print(W1.shape, b1.shape, W2.shape, b2.shape)\n",
    "    W1 = W1.reshape((NUM_INPUT, NUM_HIDDEN)) #please look at this\n",
    "    b1 = b1.reshape(NUM_HIDDEN)\n",
    "    W2 = W2.reshape((NUM_HIDDEN, NUM_OUTPUT))\n",
    "    b2 = b2.reshape(NUM_OUTPUT)\n",
    "    print(W1.shape, b1.shape, W2.shape, b2.shape)\n",
    "    return W1, b1, W2, b2\n",
    "\n",
    "# Given individual weights and biases W1, b1, W2, b2, concatenate them and\n",
    "# return a vector w containing all of them.\n",
    "# This is useful for performing a gradient check with check_grad.\n",
    "def pack (W1, b1, W2, b2):\n",
    "    W1 = W1.reshape((-1,1)) #please look at this\n",
    "    b1 = b1.reshape((-1,1))\n",
    "    W2 = W2.reshape((-1,1))\n",
    "    b2 = b2.reshape((-1,1))\n",
    "    w = (W1, b1, W2, b2) # is this what he wants?\n",
    "    return w\n",
    "\n",
    "# Load the images and labels from a specified dataset (train or test).\n",
    "def loadData (which):\n",
    "    images = np.load(\"{}mnist_{}_images.npy\".format(data_path,which))\n",
    "    labels = np.load(\"{}mnist_{}_labels.npy\".format(data_path,which))\n",
    "    return images, labels\n",
    "\n",
    "# Given training images X, associated labels Y, and a vector of combined weights\n",
    "# and bias terms w, compute and return the cross-entropy (CE) loss. You might\n",
    "# want to extend this function to return multiple arguments (in which case you\n",
    "# will also need to modify slightly the gradient check code below).\n",
    "def fCE (X, Y, w):\n",
    "    (W1, b1, W2, b2) = w #unpack(w)\n",
    "    z1 = compute_z(W1, X, b1)\n",
    "    h1 = reLU(z1)\n",
    "    z2 = compute_z(W2, h1, b2)\n",
    "    Yhat = softmax(z2)\n",
    "    cost = -np.mean(np.log(Yhat[Y==1]))\n",
    "    print(cost)\n",
    "    return cost\n",
    "\n",
    "def compute_z(W, x, b):\n",
    "    return x.dot(W) + b\n",
    "\n",
    "def reLU(s):\n",
    "    return max(0, s)\n",
    "\n",
    "def reLUPrime(s):\n",
    "    return s>0\n",
    "\n",
    "reLU = np.vectorize(reLU) # vectorizing the i/o by applying reLU func\n",
    "reLUPrime = np.vectorize(reLUPrime) # vectorizing the i/o by applying reLUPrime func\n",
    "\n",
    "def softmax(z):\n",
    "    # this is how i did in DATA MINING\n",
    "    # e_z = np.exp(z - np.max(z))\n",
    "    # return e_z/e_z.sum()\n",
    "\n",
    "    # this is how i did in HW3 - not sure which one is correct\n",
    "    e_z = np.exp(z) \n",
    "    return e_z/np.sum(e_z, axis=0)\n",
    "\n",
    "# Given training images X, associated labels Y, and a vector of combined weights\n",
    "# and bias terms w, compute and return the gradient of fCE. You might\n",
    "# want to extend this function to return multiple arguments (in which case you\n",
    "# will also need to modify slightly the gradient check code below).\n",
    "def gradCE (X, Y, w):\n",
    "    (W1, b1, W2, b2) = w #unpack(w)\n",
    "    z1 = compute_z(W1, X, b1) \n",
    "    h1 = reLU(z1)\n",
    "    z2 = compute_z(W2, h1, b2)\n",
    "    Yhat = softmax(z2)\n",
    "    \n",
    "    g = ((Yhat-Y).dot(W2.T)).dot(reLUPrime(z1.T)).T\n",
    "    dW1 = g.dot(X)\n",
    "    db1 = np.mean(g, axis=1).reshape(-1,1)\n",
    "\n",
    "    dW2 = (Yhat-Y).T.dot(h1)\n",
    "    db2 = np.mean(Yhat - Y, axis=1).reshape(-1,1)\n",
    "#     print(dW1.shape, db1.shape, dW2.shape, db2.shape)\n",
    "    \n",
    "    return (dW1.T,db1,dW2.T,db2)\n",
    "\n",
    "\n",
    "# Given training and testing datasets and an initial set of weights/biases b,\n",
    "# train the NN. Then return the sequence of w's obtained during SGD.\n",
    "def train (trainX, trainY, testX, testY, w):\n",
    "#     W1, b1, W2, b2 = unpack(w)\n",
    "#     ws = (W1, b1, W2, b2)\n",
    "\n",
    "    ws = w # bad ref?\n",
    "    lr = 0.0001\n",
    "    epochs = 5\n",
    "    \n",
    "    for i in range(epochs):\n",
    "        # TODO mini batches here\n",
    "        grads = gradCE(trainX[:64], trainY[:64], w)\n",
    "        for x in range(len(ws)):\n",
    "            ws[x] = ws[x] - lr*grads[x] # I think this should be vecotirzed outside a loop somehow\n",
    "        \n",
    "        print(ws)\n",
    "        \n",
    "    return ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (784,40) (784,64) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-2fb0cb0c0c7b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;31m# Train the network and obtain the sequence of w's obtained using SGD.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mW1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m \u001b[0mws\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-19-1f2f002af5d4>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(trainX, trainY, testX, testY, w)\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgradCE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainY\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mws\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m             \u001b[0mws\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mws\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# I think this should be vecotirzed outside a loop somehow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mws\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (784,40) (784,64) "
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "if \"trainX\" not in globals():\n",
    "    trainX, trainY = loadData(\"train\")\n",
    "    testX, testY = loadData(\"test\")\n",
    "\n",
    "# Initialize weights randomly\n",
    "W1 = 2*(np.random.random(size=(NUM_INPUT, NUM_HIDDEN))/NUM_INPUT**0.5) - 1./NUM_INPUT**0.5\n",
    "b1 = 0.01 * np.ones(NUM_HIDDEN)\n",
    "W2 = 2*(np.random.random(size=(NUM_HIDDEN, NUM_OUTPUT))/NUM_HIDDEN**0.5) - 1./NUM_HIDDEN**0.5\n",
    "b2 = 0.01 * np.ones(NUM_OUTPUT)\n",
    "# w = pack(W1, b1, W2, b2)\n",
    "\n",
    "# if DEBUG:\n",
    "#     print(\"Shape of Image: {}\".format(trainX.shape))\n",
    "#     print(\"Shape of Label: {}\".format(trainY.shape))\n",
    "#     print(\"Shape of W1: {}\".format(W1.shape))\n",
    "#     print(\"Shape of b1: {}\".format(b1.shape))\n",
    "#     print(\"Shape of W2: {}\".format(W2.shape))\n",
    "#     print(\"Shape of b2: {}\".format(b2.shape))\n",
    "\n",
    "# Check that the gradient is correct on just a few examples (randomly drawn).\n",
    "# idxs = np.random.permutation(trainX.shape[0])[0:NUM_CHECK]\n",
    "\n",
    "# packed_w = gradCE(np.atleast_2d(trainX[idxs,:]), np.atleast_2d(trainY[idxs,:]), w)\n",
    "\n",
    "# so sorry abt this mess i cant figure out with loops and this is faster for now\n",
    "# grad_vector = np.append(packed_w[0].flatten(), packed_w[1].flatten())\n",
    "# grad_vector = np.append(grad_vector, packed_w[2].flatten())\n",
    "# grad_vector = np.append(grad_vector, packed_w[3].flatten())\n",
    "\n",
    "# for i in packed_w:\n",
    "#     grad_vector.append(i.flatten())\n",
    "    \n",
    "# grad_vector = np.array(grad_vector)\n",
    "# print(grad_vector.shape)\n",
    "# print(scipy.optimize.check_grad(fCE(np.atleast_2d(trainX[idxs,:]), np.atleast_2d(trainY[idxs,:]), w), \n",
    "#                                gradCE(np.atleast_2d(trainX[idxs,:]), np.atleast_2d(trainY[idxs,:]), w), [1]))\n",
    "\n",
    "\n",
    "# print(scipy.optimize.check_grad(lambda w_: fCE(np.atleast_2d(trainX[idxs,:]), np.atleast_2d(trainY[idxs,:]), w_), \\\n",
    "#                                 lambda w_: gradCE(np.atleast_2d(trainX[idxs,:]), np.atleast_2d(trainY[idxs,:]), w_), \\\n",
    "#                                 w))\n",
    "\n",
    "# Train the network and obtain the sequence of w's obtained using SGD.\n",
    "w = (W1, b1, W2, b2)\n",
    "ws = train(trainX, trainY, testX, testY, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
